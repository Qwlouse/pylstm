{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pylstm import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Quickstart"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build and initialize a simple network\n",
      "net = build_net(InputLayer(3) >> ForwardLayer(2, act_func='sigmoid'))\n",
      "net.error_func = CrossEntropyError\n",
      "net.initialize(default=Gaussian(std=0.1))\n",
      "\n",
      "# Randomly generate a dataset\n",
      "X = np.random.randn(1, 100, 3) # nr_timesteps, batch_size, input_size\n",
      "T = np.random.random_sample((1, 100, 2))\n",
      "\n",
      "# Set up an SGD trainer that stops after 10 epochs\n",
      "tr = Trainer(net, SGDStep(learning_rate=0.01))\n",
      "tr.stopping_criteria.append(MaxEpochsSeen(10))\n",
      "\n",
      "# Train with weight updates after each sample\n",
      "tr.train(Online(X, T))\n",
      "\n",
      "plot(tr.training_errors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'build_net' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-064cde05601e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Build and initialize a simple network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>>\u001b[0m \u001b[0mForwardLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGaussian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'build_net' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Building a Network\n",
      "\n",
      "A network is built by connecting Layers to each other (using the >> operator) and then calling build_net() on one of them.\n",
      "\n",
      "You always need an InputLayer, but the output layer is inferred from the topology."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net = build_net(InputLayer(3) >> ForwardLayer(3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Architecture\n",
      "A network can be inspected in a variety of ways. We only look at the architecture for now. That gives you a list of dictionaries, one for each layer of the network. There you can see all relevant information about the layer:\n",
      "\n",
      "- name: the unique name of the layer\n",
      "- type: the type of the layer\n",
      "- size: number of units/neurons\n",
      "- targets: the names of the layers this layer connects to\n",
      "- kwargs: [optional] the keyword arguments used to create this layer \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.architecture"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Examples\n",
      "So lets do some more complicated architectures:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# multiple layers of the same type get assigned unique names:\n",
      "net = build_net(InputLayer(7) >> ForwardLayer(8) >> ForwardLayer(9) >> ForwardLayer(10))\n",
      "net.architecture"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# But you can also assign names yourself if you like:\n",
      "net = build_net(InputLayer(7) >> ForwardLayer(8, name='foo') >> ForwardLayer(9, name='bar') >> ForwardLayer(10, name='baz'))\n",
      "net.architecture"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can split constructing the layers, wiring them up, and building the network as you like:\n",
      "il = InputLayer(7)\n",
      "foo = ForwardLayer(8, name='foo')\n",
      "bar = ForwardLayer(9, name='bar')\n",
      "baz = ForwardLayer(10, name='baz')\n",
      "il >> foo >> bar >> baz\n",
      "net = build_net(il) # == build_net(foo) == build_net(bar) == build_net(baz)\n",
      "net.architecture"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can build more complicated architectures using this technique. Here an BLSTM example:\n",
      "il = InputLayer(7)\n",
      "ol = ForwardLayer(9)\n",
      "il >> LstmLayer(8) >> ol\n",
      "il >> ReverseLayer() >> LstmLayer(8) >> ReverseLayer() >> ol\n",
      "# this line ^ is equivalent to the line below\n",
      "# il >> ReverseLayer(7) >> LstmLayer(8) >> ReverseLayer(8) >> ol\n",
      "net = build_net(il)\n",
      "net.architecture\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Layer Types"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "il = InputLayer()\n",
      "\n",
      "fwd = ForwardLayer()\n",
      "\n",
      "rnn = RnnLayer()\n",
      "mrnn = MrnnLayer()\n",
      "arnn = ArnnLayer()\n",
      "lstm = LstmLayer()\n",
      "lstm97 = Lstm97Layer()\n",
      "\n",
      "rev = ReverseLayer()\n",
      "noop = NoOpLayer()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Layer Construction Parameters\n",
      "You can pass some parameters to layer construction. They will show up in the architecture as kwargs. The activation function is a very important parameter and can be used with most layer types (save InputLayer, ReverseLayer, NoOpLayer)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net = build_net(InputLayer(3) >> ForwardLayer(3, act_func='softmax'))\n",
      "net.architecture"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So lets see some more exotic ones. The following parameters are special to the corresponding layer type:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fl = ForwardLayer(3, use_bias=False) # don't use biases\n",
      "\n",
      "lstm = LstmLayer(4, delta_range=1.0) # clip the unit activation deltas to the range [-1.0, 1.0] \n",
      "\n",
      "lstm97 = Lstm97Layer(5, \n",
      "                     full_gradient=False,         # Use truncated gradient\n",
      "                     peephole_connections=False,  # don't use peephole connections\n",
      "                     forget_gate=False,           # don't include a forget gate\n",
      "                     output_gate=False,           # don't include an output gate\n",
      "                     gate_recurrence=True,        # have recurrent connections among the gates\n",
      "                     use_bias=True)               # use biases for all units\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Initialization\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net = build_net(InputLayer(2) >> RnnLayer(3) >> ForwardLayer(4, name='OutputLayer')) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The weights of a network are stored in it's param_buffer. But be careful: The param buffer needs to be initialized!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# net.param_buffer   < this will fail\n",
      "net.initialize()\n",
      "net.param_buffer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## initialize()\n",
      "This param_buffer can be assigned by hand, but that is not recommended. For initialization the easiest way is to use the initialize method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.initialize(Uniform(low=-0.2, high=0.2))\n",
      "# net.initialize(default=Uniform(low=-0.2, high=0.2))      # equivalent\n",
      "# net.initialize({'default':Uniform(low=-0.2, high=0.2)})  # equivalent\n",
      "\n",
      "net.param_buffer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This will also give you easy access to the weights  that belong to the different layers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.initialize(default=0, OutputLayer=1, RnnLayer=2)\n",
      "net.param_buffer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or even to the different weight matrices that every layer uses:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.initialize(default=0, \n",
      "           OutputLayer={'HX':1, 'H_bias':2}, \n",
      "           RnnLayer={'HX':3, 'HR':4, 'H_bias':5})\n",
      "net.param_buffer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Initialization types\n",
      "For initializing you can either use one of the initializers (Uniform, Gaussian) or anything that can be converted to a numpy array and then broadcasts to the correct shape:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.initialize(default=0, \n",
      "           RnnLayer={'HR':Uniform(0, 1)})\n",
      "net.param_buffer  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.initialize(default=0, \n",
      "           RnnLayer={'HR':1})\n",
      "net.param_buffer     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.initialize(default=0, \n",
      "           RnnLayer={'HR':[1, 2, 3]})\n",
      "net.param_buffer         \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.initialize(default=0, \n",
      "           RnnLayer={'HR':[[1, 2, 3], [4, 5, 6], [7, 8, 9]]})\n",
      "net.param_buffer     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.initialize(Gaussian())\n",
      "net.param_buffer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build and initialize a simple network\n",
      "net = build_net(InputLayer(3) >> ForwardLayer(2, act_func='sigmoid'))\n",
      "net.error_func = CrossEntropyError\n",
      "net.initialize(default=Gaussian(std=0.1))\n",
      "\n",
      "# Randomly generate a dataset\n",
      "X = np.random.randn(1, 100, 3) # nr_timesteps, batch_size, input_size\n",
      "T = np.random.random_sample((1, 100, 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training is done by the Trainer (suprise!). You have to specify a network when you set it up:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr = Trainer(net)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Stopping Criteria\n",
      "You need to specify a stopping criterion if you want the training to stop. Here we tell it to stop after 10 Epochs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr.stopping_criteria.append(MaxEpochsSeen(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another common stopping criteria is ValidationErrorRises()"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tr.stopping_criteria.append(ValidationErrorRises())   # need to pass validation data to train() method"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data-iterators\n",
      "The train() method takes a data-iterator as argument. There are 3 different data-iterators so far: Undivided, Minibatches, and Online. \n",
      "\n",
      "They specify how the dataset is split up during each epoch to do the weight-updates. So Undivided gives you Gradient Descent while Online gives you Stochastic Gradient Descent:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr.train(Undivided(X, T))\n",
      "# tr.train(Minibatches(X, T, batch_size=10))\n",
      "# tr.train(Online(X, T))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data-iterators can optionally also work with a Mask, but that is only important for sequences."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Steps\n",
      "The trainer can be configured to perform other operations than Gradient Descent by passing in a Step Object during initialization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr = Trainer(net, SgdStep(learning_rate=0.2))  # (Stochastic) Gradient Descent\n",
      "tr = Trainer(net, MomentumStep(learning_rate=0.1, momentum=0.9)) # SGD with momentum\n",
      "tr = Trainer(net, NesterovStep(learning_rate=0.1, momentum=0.9)) # Nesterovs variant of momentum\n",
      "tr = Trainer(net, RPropStep(learning_rate=0.1)) # RProp (untested)\n",
      "\n",
      "# two special steps exist that you probably won't need\n",
      "tr = Trainer(net, DiagnosticStep()) # Don't train just print debugging info\n",
      "tr = Trainer(net, ForwardStep()) # Don't train, just run a forward pass\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Callbacks\n",
      "To have the trainer perform certain operations after each epoch you can specify callback functions like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tr.callbacks.append(print_error_per_epoch)     # This function is added by default, so don't worry about it\n",
      "tr.callbacks.append(SaveWeightsPerEpoch(\"weights.npy\"))  # This will save the weights of your net after each epoch\n",
      "tr.callbacks.append(SaveBestWeights(\"best_weights.npy\")) # This will only save the weights if the error decreased"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the builtin callbacks but you can easily define your own. The callback has to be callable and take the following arguments:\n",
      "\n",
      "- epoch: the current epoch number\n",
      "- net: the network\n",
      "- training_errors: a list of all training errors so far\n",
      "- validation_errors: a list of all validation errors so far (if any)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Custom callback as Function:\n",
      "def custom_callback_func(epoch, net, training_error, validation_error):\n",
      "    print('Puh, epoch %d was hard!'%epoch)\n",
      "    \n",
      "tr.callbacks.append(custom_callback_func)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Custom callback as Class:\n",
      "class CustomCallback(object):\n",
      "    def __init__(self, message):\n",
      "        self.message = message\n",
      "    \n",
      "    def __call__(self, epoch, net, training_error, validation_error):\n",
      "        print(self.message)\n",
      "        \n",
      "tr.callbacks.append(CustomCallback('Hello World!'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}